{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling For the WeRateDogs on Twitter Data\n",
    "The following projected consisted of three primary data wrangling steps in this order: gather, assess, and clean.\n",
    "\n",
    "\n",
    "### Data Gathering\n",
    "Acquiring the data for analysis in this projected consisted of getting data from three different sources. The first file, twitter_archive_enhanced.csv, was a comma-separated values (csv) file that was downloaded by clicking on a link provided by Udacity. The process is the same as would be when downloading a csv file off Kaggle or some other website that hosts datasets as csv files. Once downloaded, the file is uploaded as a Pandas dataframe. The next file, image_predictions.tsv, file was downloaded off Udacity's servers. This file was programmatically downloaded using the Requests library. The process included creating a folder and then using the Requests library to download the file and with some short code, opening the file in the newly created folder. Gathering data from the third source involved setting up a Twitter developer account and being granted access permissions. The Tweepy library was then used to extract the data from the Twitter API for each tweet's JSON data. A list was created by using the Twitter API that would then be stored into JSON text file called 'tweet_data_json.txt'. This file was then uploaded using Pandas into a dataframe.\n",
    "\n",
    "### Assessing the Data.\n",
    "This process included visually assessing the data and programmatically assessing the data. Assessing the data involved using some basic level code to quickly observe select rows from each data set looking for any quality or tidiness issues. Visually assessing the data provided some quick clues as to what some of the quality and tidiness issues were. To further assess the data, involved doing so programmatically. This process also utilized some basic code operations to assess for accuracy, validity, consistency, and completeness of the data. Marking down specific data quality and data tidiness issues was performed in this step of the process. \n",
    "\n",
    "### Data cleaning\n",
    "Before beginning the data cleaning process, a copy of each of the three dataframes that were created by assessing and downloaded through the steps above was performed. Data cleaning was a three-step process. The first step in the process was defining the how the quality or tidiness issue was going to be fixed. This sometimes included the description of the code that would be used in fixing the issue. The second step was implementing the code itself. The final step was testing out our code to make sure the quality or tidiness issue was resolved correctly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
